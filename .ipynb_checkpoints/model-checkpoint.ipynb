{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - C5i Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Yaxh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.corpus import words\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Datatset loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "First 5 rows of the dataset:\n",
      "   Unique ID          Start Date            End Date  \\\n",
      "0        NaN                 NaT                 NaT   \n",
      "1        3.0 2024-10-11 09:43:37 2024-10-11 09:53:40   \n",
      "2        5.0 2024-10-11 09:42:41 2024-10-11 09:55:49   \n",
      "3        6.0 2024-10-11 09:46:20 2024-10-11 09:56:42   \n",
      "4        8.0 2024-10-11 09:47:15 2024-10-11 09:57:19   \n",
      "\n",
      "   Q1. What is your current age? \\n(Age)  Q2. What is your gender? \\n(Gender)  \\\n",
      "0                                    NaN                                  NaN   \n",
      "1                                   60.0                                  2.0   \n",
      "2                                   61.0                                  1.0   \n",
      "3                                   58.0                                  1.0   \n",
      "4                                   55.0                                  1.0   \n",
      "\n",
      "   Q3. Which of the following best describes the area or community in which you live? \\n(Urban/Rural)  \\\n",
      "0                                                NaN                                                    \n",
      "1                                                3.0                                                    \n",
      "2                                                3.0                                                    \n",
      "3                                                1.0                                                    \n",
      "4                                                3.0                                                    \n",
      "\n",
      "   Q4.  Please indicate the answer that includes your entire household income in (previous year) before taxes. \\n(Income)  \\\n",
      "0                                                NaN                                                                        \n",
      "1                                               12.0                                                                        \n",
      "2                                                3.0                                                                        \n",
      "3                                                8.0                                                                        \n",
      "4                                               11.0                                                                        \n",
      "\n",
      "  Q6 Which of the following types of alcoholic beverages have you consumed in the past 4 weeks?\\n(Alcohol Category)  \\\n",
      "0                                               Beer                                                                  \n",
      "1                                                  1                                                                  \n",
      "2                                                  1                                                                  \n",
      "3                                                  1                                                                  \n",
      "4                                                  1                                                                  \n",
      "\n",
      "             Unnamed: 8            Unnamed: 9  ...  \\\n",
      "0   Flavored/Mixed Beer   Non-Alcoholic Beers  ...   \n",
      "1                     0                     0  ...   \n",
      "2                     0                     0  ...   \n",
      "3                     0                     0  ...   \n",
      "4                     0                     0  ...   \n",
      "\n",
      "  Q13. How does the price fit with what youâ€™d expect the shown to cost?\\n(Concept_Price)  \\\n",
      "0                                                NaN                                       \n",
      "1                                                3.0                                       \n",
      "2                                                3.0                                       \n",
      "3                                                3.0                                       \n",
      "4                                                3.0                                       \n",
      "\n",
      "  Q14. Which statement below best describes how likely you would be to buy shown product if it were available at your local stores?\\n(Concept_Purchase Intent)  \\\n",
      "0                                                NaN                                                                                                             \n",
      "1                                                2.0                                                                                                             \n",
      "2                                                5.0                                                                                                             \n",
      "3                                                1.0                                                                                                             \n",
      "4                                                5.0                                                                                                             \n",
      "\n",
      "  Q15. If the shwon product was available to you, how often would you expect yourself to drink at least one of these products?\\n(Concept_Drinking Frequency)  \\\n",
      "0                                                NaN                                                                                                           \n",
      "1                                                2.0                                                                                                           \n",
      "2                                                4.0                                                                                                           \n",
      "3                                                0.0                                                                                                           \n",
      "4                                                7.0                                                                                                           \n",
      "\n",
      "  Q16A. What is the most important thing you LIKE about the shown concept}?     This can include anything you would want kept for sure or aspects that might drive you to buy or try itâ€¦       Please type a detailed response in the space below  \\\n",
      "0                                                NaN                                                                                                                                                                                                \n",
      "1                                      the packaging                                                                                                                                                                                                \n",
      "2                     It's a good and trusted brand.                                                                                                                                                                                                \n",
      "3                       Just that it is a lager beer                                                                                                                                                                                                \n",
      "4                          has a less filling effect                                                                                                                                                                                                \n",
      "\n",
      "  Q16B. What is the most important thing you DISLIKE about the shown concept}?    This can include general concerns, annoyances, or any aspects of the product that need fixed for this to be more appealing to you...     Please type a detailed response in the space below.  \\\n",
      "0                                                NaN                                                                                                                                                                                                                             \n",
      "1                                            nothing                                                                                                                                                                                                                             \n",
      "2                       It's seem some what generic.                                                                                                                                                                                                                             \n",
      "3  I don't like Anheiser Busch and their values a...                                                                                                                                                                                                                             \n",
      "4                                         no dislike                                                                                                                                                                                                                             \n",
      "\n",
      "  Q17. We would like to know what effect this new product might have on the other beverages you buy. If it were available, would the shown productâ€¦? \\n(Concept_Replacement Product)  \\\n",
      "0                                                NaN                                                                                                                                   \n",
      "1                                                1.0                                                                                                                                   \n",
      "2                                                2.0                                                                                                                                   \n",
      "3                                                1.0                                                                                                                                   \n",
      "4                                                2.0                                                                                                                                   \n",
      "\n",
      "  Q18_1 What specific product that you are currently using would the shown product replace?\\n Please type in ONE specific brand or product per space provided.  \\\n",
      "0                                                NaN                                                                                                             \n",
      "1                                                NaN                                                                                                             \n",
      "2                                           Heineken                                                                                                             \n",
      "3                                                NaN                                                                                                             \n",
      "4                                     michelob ultra                                                                                                             \n",
      "\n",
      "  Q18_2 What specific product that you are currently using would the shown concept replace?\\n Please type in ONE specific brand or product per space provided.  \\\n",
      "0                                                NaN                                                                                                             \n",
      "1                                                NaN                                                                                                             \n",
      "2                                        Miller Lite                                                                                                             \n",
      "3                                                NaN                                                                                                             \n",
      "4                                       Miller light                                                                                                             \n",
      "\n",
      "  Q18_3 What specific product that you are currently using would the shown concept replace?\\n Please type in ONE specific brand or product per space provided.  \\\n",
      "0                                                NaN                                                                                                             \n",
      "1                                                NaN                                                                                                             \n",
      "2                                          Budwieser                                                                                                             \n",
      "3                                                NaN                                                                                                             \n",
      "4                                             corona                                                                                                             \n",
      "\n",
      "  OE_Quality_Flag  \n",
      "0             NaN  \n",
      "1             0.0  \n",
      "2             0.0  \n",
      "3             0.0  \n",
      "4             0.0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(r\"Final Data File_Training.xlsx\", sheet_name=1)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Gibberish Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\2800129156.py:50: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  df['Gibberish_Flag'] = df.iloc[:, oe_columns].apply(lambda row: max(is_gibberish(row[0]), is_gibberish(row[1])), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved successfully with 'Gibberish_Flag' column at the end.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "def is_gibberish(text, entropy_threshold=3.46, keyboard_seq_length=4, min_valid_word_ratio=0.3):\n",
    "    \"\"\"Detect gibberish text based on entropy, keyboard patterns, and valid word ratio.\"\"\"\n",
    "    \n",
    "    if pd.isna(text) or len(str(text).strip()) < 4:\n",
    "        return 0  # Not gibberish if empty or too short\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    clean_text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    if len(clean_text.strip()) < 3:\n",
    "        return 0  # Not gibberish if it's too short after cleaning\n",
    "\n",
    "    tokens = clean_text.split()\n",
    "    valid_words = sum(1 for word in tokens if word in english_words)\n",
    "    word_ratio = valid_words / len(tokens) if tokens else 0\n",
    "\n",
    "    if word_ratio >= min_valid_word_ratio:\n",
    "        return 0  # Not gibberish if enough words are valid\n",
    "\n",
    "    # Character entropy calculation\n",
    "    char_counts = Counter(text)\n",
    "    text_length = len(text)\n",
    "    entropy = -sum((count/text_length) * math.log2(count/text_length) for count in char_counts.values())\n",
    "\n",
    "    # Check for keyboard sequences\n",
    "    keyboard_rows = ['qwertyuiop', 'asdfghjkl', 'zxcvbnm']\n",
    "    has_keyboard = any(\n",
    "        any(text[i:i+keyboard_seq_length] in row or text[i:i+keyboard_seq_length] in row[::-1]\n",
    "            for row in keyboard_rows)\n",
    "        for i in range(len(text) - keyboard_seq_length + 1)\n",
    "    )\n",
    "\n",
    "    # Check for repeating characters\n",
    "    has_repeats = any(c * 3 in text for c in set(text))\n",
    "\n",
    "    # Check for consecutive consonants or vowels\n",
    "    consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "    vowels = 'aeiou'\n",
    "    has_consecutive = re.search(r'[' + consonants + ']{5}|[' + vowels + ']{4}', clean_text) is not None\n",
    "\n",
    "    return 1 if (entropy > entropy_threshold or has_keyboard or has_repeats or has_consecutive) else 0\n",
    "\n",
    "\n",
    "oe_columns = [29, 30]\n",
    "\n",
    "# Apply gibberish detection and add a new column\n",
    "df['Gibberish_Flag'] = df.iloc[:, oe_columns].apply(lambda row: max(is_gibberish(row[0]), is_gibberish(row[1])), axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel(\"new_yay_updated.xlsx\", sheet_name=\"Sheet1\", index=False)\n",
    "\n",
    "print(\"Updated dataset saved successfully with 'Gibberish_Flag' column at the end.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - OffTopic Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  substring_mask = processed_col.str.contains(valid_substrings, na=False)\n",
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:58: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fuzzy_mask = fuzzy_candidates.apply(rapidfuzz_match).replace({0: True, 1: False})\n",
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  substring_mask = processed_col.str.contains(valid_substrings, na=False)\n",
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:58: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fuzzy_mask = fuzzy_candidates.apply(rapidfuzz_match).replace({0: True, 1: False})\n",
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:55: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  substring_mask = processed_col.str.contains(valid_substrings, na=False)\n",
      "C:\\Users\\Yaxh\\AppData\\Local\\Temp\\ipykernel_17076\\3743088491.py:58: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  fuzzy_mask = fuzzy_candidates.apply(rapidfuzz_match).replace({0: True, 1: False})\n"
     ]
    }
   ],
   "source": [
    "#### filepath: c:\\Users\\Yaxh\\Desktop\\Hackathonn\\model.ipynb\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from rapidfuzz import process, fuzz  # Faster than fuzzywuzzy\n",
    "from rapidfuzz.utils import default_process\n",
    "\n",
    "\n",
    "\n",
    "combined_df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "\n",
    "valid_entries = set()\n",
    "for _, row in combined_df.iterrows():\n",
    "    brand = str(row[\"Brand\"]).strip().lower()\n",
    "    product = str(row[\"Product\"]).strip().lower()\n",
    "    valid_entries.update([brand, product])\n",
    "valid_entries = list(valid_entries)\n",
    "\n",
    "# Create a regex pattern for substring matching\n",
    "valid_substrings = re.compile(\n",
    "    r\"\\b(\" + \"|\".join(map(re.escape, valid_entries)) + r\")\\b\", flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Vectorized preprocessing\n",
    "def preprocess_column(col):\n",
    "    return (\n",
    "        col.astype(str)\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[^a-z0-9\\s]\", \" \", regex=True)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "# Preprocess valid entries once for rapidfuzz\n",
    "preprocessed_valid = [default_process(entry) for entry in valid_entries]\n",
    "\n",
    "def rapidfuzz_match(entry):\n",
    "    processed_entry = default_process(entry)\n",
    "    # First check exact match\n",
    "    if processed_entry in preprocessed_valid:\n",
    "        return 0\n",
    "    # Then fuzzy match with threshold 85\n",
    "    result = process.extractOne(processed_entry, preprocessed_valid, scorer=fuzz.ratio, score_cutoff=85)\n",
    "    return 0 if result else 1\n",
    "\n",
    "# Main flagging logic (example columns 32, 33, 34)\n",
    "beer_columns = df.columns[32:35]\n",
    "\n",
    "for col in beer_columns:\n",
    "    processed_col = preprocess_column(df[col])\n",
    "    \n",
    "    # 1) Exact matches\n",
    "    exact_mask = processed_col.isin(valid_entries)\n",
    "    # 2) Substring matches\n",
    "    substring_mask = processed_col.str.contains(valid_substrings, na=False)\n",
    "    # 3) Fuzzy matches for remaining\n",
    "    fuzzy_candidates = processed_col[~(exact_mask | substring_mask)]\n",
    "    fuzzy_mask = fuzzy_candidates.apply(rapidfuzz_match).replace({0: True, 1: False})\n",
    "    \n",
    "    # Combine flags (0 = valid, 1 = flagged)\n",
    "    final_mask = exact_mask | substring_mask | fuzzy_mask\n",
    "    df[f\"{col}_flag\"] = np.where(final_mask, 0, 1)\n",
    "\n",
    "df.to_excel(\"flagged_data_optimized.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - AI detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AI detection completed! Results saved to output_with_ai_detection.csv\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Load AI detection model\n",
    "detector = pipeline(\"text-classification\", model=\"roberta-base-openai-detector\")\n",
    "\n",
    "# AI Detection Function\n",
    "def detect_ai(text):\n",
    "    if pd.isna(text) or len(text) <= 5:\n",
    "        return \"Human-Written\"\n",
    "    \n",
    "    result = detector(text)[0]\n",
    "    return \"AI-Generated\" if result['label'] == 'LABEL_1' else \"Human-Written\"\n",
    "\n",
    "# Apply detection to columns 29 and 30\n",
    "df[\"AI_Detection_29\"] = df.iloc[:, 29].apply(detect_ai)\n",
    "df[\"AI_Detection_30\"] = df.iloc[:, 30].apply(detect_ai)\n",
    "\n",
    "# Check for any AI-generated text\n",
    "if (df[\"AI_Detection_29\"] == \"AI-Generated\").any() or (df[\"AI_Detection_30\"] == \"AI-Generated\").any():\n",
    "    print(\"AI-generated text detected!\")\n",
    "\n",
    "# Save the results\n",
    "df.to_csv(\"output_with_ai_detection.csv\", index=False)\n",
    "\n",
    "print(\"âœ… AI detection completed! Results saved to output_with_ai_detection.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - S-Bert transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ **Test Accuracy: 0.9033**\n",
      "\n",
      "ðŸ”¹ **Classification Report:**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92       200\n",
      "           1       0.78      1.00      0.87       100\n",
      "\n",
      "    accuracy                           0.90       300\n",
      "   macro avg       0.89      0.93      0.90       300\n",
      "weighted avg       0.93      0.90      0.91       300\n",
      "\n",
      "\n",
      "âœ… **Test Results saved to 'classified_responses_sbert_balanced_test.xlsx'**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load dataset and clean column names\n",
    "def load_data_xlsx(file_path, sheet_name=\"Data Set with Labels Text\"):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        \"Q16A. What is the most important thing you LIKE about the shown concept}?     This can include anything you would want kept for sure or aspects that might drive you to buy or try itâ€¦       Please type a detailed response in the space below\":\n",
    "            \"Q16A_Likes\",\n",
    "        \"Q16B. What is the most important thing you DISLIKE about the shown concept}?    This can include general concerns, annoyances, or any aspects of the product that need fixed for this to be more appealing to you...     Please type a detailed response in the space below.\":\n",
    "            \"Q16B_Dislikes\",\n",
    "        \"OE_Quality_Flag\": \"Quality_Flag\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "# Prepare combined text feature\n",
    "def combine_texts(row):\n",
    "    # If any response is missing, fill with \"missing_text\"\n",
    "    text_a = str(row[\"Q16A_Likes\"]) if pd.notna(row[\"Q16A_Likes\"]) else \"missing_text\"\n",
    "    text_b = str(row[\"Q16B_Dislikes\"]) if pd.notna(row[\"Q16B_Dislikes\"]) else \"missing_text\"\n",
    "    return text_a + \" \" + text_b\n",
    "\n",
    "# Train classifier on combined text features using SBERT embeddings\n",
    "def train_classifier(texts, labels):\n",
    "    # Load SBERT model and encode text\n",
    "    sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    embeddings = sbert_model.encode(texts)\n",
    "    \n",
    "    # Scale embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Train Logistic Regression with balanced class weights\n",
    "    clf = LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=1000, C=1.0)\n",
    "    clf.fit(embeddings_scaled, labels)\n",
    "    \n",
    "    return clf, scaler, sbert_model\n",
    "\n",
    "# Evaluate classifier on new texts\n",
    "def evaluate_classifier(clf, scaler, sbert_model, texts, threshold=0.5):\n",
    "    embeddings = sbert_model.encode(texts)\n",
    "    embeddings_scaled = scaler.transform(embeddings)\n",
    "    \n",
    "    # Get predicted probabilities (for class 1)\n",
    "    pred_probs = clf.predict_proba(embeddings_scaled)[:, 1]\n",
    "    predicted_labels = (pred_probs > threshold).astype(int)\n",
    "    return predicted_labels\n",
    "\n",
    "# Main function for training on full dataset, testing on balanced set\n",
    "def main(file_path, sheet_name):\n",
    "    df = load_data_xlsx(file_path, sheet_name)\n",
    "    \n",
    "    # Drop rows with missing Quality_Flag and reset index\n",
    "    df = df.dropna(subset=[\"Quality_Flag\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Convert Quality_Flag to binary labels: assume 1 or '1.0' as Bad, else Good (0)\n",
    "    df[\"Quality_Flag_Binary\"] = df[\"Quality_Flag\"].apply(lambda x: 1 if x in [1, '1.0'] else 0)\n",
    "    \n",
    "    # Combine Q16A and Q16B responses into one feature\n",
    "    df[\"Combined_Text\"] = df.apply(combine_texts, axis=1)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = shuffle(df, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split dataset for training and testing\n",
    "    # Use the whole dataset for training\n",
    "    train_df = df.copy()\n",
    "\n",
    "    # Create a balanced test dataset (equal number of 0s and 1s)\n",
    "    test_good = df[df[\"Quality_Flag_Binary\"] == 0].sample(n=200, random_state=42)  \n",
    "    test_bad  = df[df[\"Quality_Flag_Binary\"] == 1].sample(n=100, random_state=42)  \n",
    "    test_df = pd.concat([test_good, test_bad]).reset_index(drop=True)\n",
    "    \n",
    "    # Train classifier on combined text from the full dataset\n",
    "    train_texts = train_df[\"Combined_Text\"].fillna(\"missing_text\").astype(str).tolist()\n",
    "    train_labels = train_df[\"Quality_Flag_Binary\"].tolist()\n",
    "    clf, scaler, sbert_model = train_classifier(train_texts, train_labels)\n",
    "    \n",
    "    # Evaluate on balanced test set\n",
    "    test_texts = test_df[\"Combined_Text\"].fillna(\"missing_text\").astype(str).tolist()\n",
    "    predicted_labels = evaluate_classifier(clf, scaler, sbert_model, test_texts, threshold=0.2)\n",
    "    \n",
    "    # Save predictions to test_df\n",
    "    test_df[\"Predicted_Values\"] = predicted_labels\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    accuracy = accuracy_score(test_df[\"Quality_Flag_Binary\"], test_df[\"Predicted_Values\"])\n",
    "    print(f\"\\nðŸ”¹ **Test Accuracy: {accuracy:.4f}**\\n\")\n",
    "    print(\"ðŸ”¹ **Classification Report:**\")\n",
    "    print(classification_report(test_df[\"Quality_Flag_Binary\"], test_df[\"Predicted_Values\"], zero_division=0))\n",
    "\n",
    "    \n",
    "    # Save test results (Quality_Flag and Predicted_Values) to Excel\n",
    "    output_df = test_df[[\"Quality_Flag\", \"Predicted_Values\"]]\n",
    "    output_file = \"classified_responses_sbert_balanced_test.xlsx\"\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "    print(f\"\\n **Test Results saved to '{output_file}'**\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    xlsx_file = \"Final Data File_Training.xlsx\"\n",
    "    sheet_name = \"Data Set with Labels Text\"\n",
    "    main(xlsx_file, sheet_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ **Test Accuracy: 0.8174**\n",
      "\n",
      "ðŸ”¹ **Classification Report:**\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.90       424\n",
      "           1       0.15      0.28      0.19        36\n",
      "\n",
      "    accuracy                           0.82       460\n",
      "   macro avg       0.54      0.57      0.54       460\n",
      "weighted avg       0.87      0.82      0.84       460\n",
      "\n",
      "\n",
      "ðŸ”¹ **Confusion Matrix:**\n",
      "   True Negative: 366, False Positive: 58\n",
      "   False Negative: 26, True Positive: 10\n",
      "\n",
      "ðŸ”¹ **Test Results saved to 'classifier_results_80_20_split.xlsx'**\n",
      "\n",
      "ðŸ”¹ Model saved to 'saved_model'\n",
      "\n",
      "ðŸ”¹ Predictions saved to 'Predicted_Results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Load dataset and clean column names\n",
    "def load_data_xlsx(file_path, sheet_name=\"Data Set with Labels Text\"):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df.rename(columns={\n",
    "        \"Q16A. What is the most important thing you LIKE about the shown concept}?     This can include anything you would want kept for sure or aspects that might drive you to buy or try itâ€¦       Please type a detailed response in the space below\":\n",
    "            \"Q16A_Likes\",\n",
    "        \"Q16B. What is the most important thing you DISLIKE about the shown concept}?    This can include general concerns, annoyances, or any aspects of the product that need fixed for this to be more appealing to you...     Please type a detailed response in the space below.\":\n",
    "            \"Q16B_Dislikes\",\n",
    "        \"OE_Quality_Flag\": \"Quality_Flag\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    return df\n",
    "\n",
    "# Prepare combined text feature\n",
    "def combine_texts(row):\n",
    "    text_a = str(row[\"Q16A_Likes\"]) if pd.notna(row[\"Q16A_Likes\"]) else \"missing_text\"\n",
    "    text_b = str(row[\"Q16B_Dislikes\"]) if pd.notna(row[\"Q16B_Dislikes\"]) else \"missing_text\"\n",
    "    return text_a + \" \" + text_b\n",
    "\n",
    "# Train classifier and save model\n",
    "def train_and_save_model(file_path, sheet_name, model_path=\"saved_model\"):\n",
    "    df = load_data_xlsx(file_path, sheet_name)\n",
    "    \n",
    "    df = df.dropna(subset=[\"Quality_Flag\"]).reset_index(drop=True)\n",
    "    df[\"Quality_Flag_Binary\"] = df[\"Quality_Flag\"].apply(lambda x: 1 if x in [1, '1.0'] else 0)\n",
    "    df[\"Combined_Text\"] = df.apply(combine_texts, axis=1)\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Quality_Flag_Binary\"])\n",
    "    \n",
    "    train_texts = train_df[\"Combined_Text\"].fillna(\"missing_text\").astype(str).tolist()\n",
    "    train_labels = train_df[\"Quality_Flag_Binary\"].tolist()\n",
    "\n",
    "    # Load SBERT model and encode text\n",
    "    sbert_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "    embeddings = sbert_model.encode(train_texts)\n",
    "\n",
    "    # Scale embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "    # Train classifier\n",
    "    clf = LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=100, C=1.0)\n",
    "    clf.fit(embeddings_scaled, train_labels)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_texts = test_df[\"Combined_Text\"].fillna(\"missing_text\").astype(str).tolist()\n",
    "    test_labels = test_df[\"Quality_Flag_Binary\"].tolist()\n",
    "    \n",
    "    test_embeddings = sbert_model.encode(test_texts)\n",
    "    test_embeddings_scaled = scaler.transform(test_embeddings)\n",
    "    \n",
    "    predicted_labels = clf.predict(test_embeddings_scaled)\n",
    "    pred_probs = clf.predict_proba(test_embeddings_scaled)[:, 1]\n",
    "\n",
    "    # Save predictions to test_df\n",
    "    test_df[\"Predicted_Values\"] = predicted_labels\n",
    "    test_df[\"Prediction_Probability\"] = pred_probs\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    print(f\"\\nðŸ”¹ **Test Accuracy: {accuracy:.4f}**\\n\")\n",
    "    print(\"ðŸ”¹ **Classification Report:**\")\n",
    "    print(classification_report(test_labels, predicted_labels, zero_division=0))\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    cm = confusion_matrix(test_labels, predicted_labels)\n",
    "    print(\"\\nðŸ”¹ **Confusion Matrix:**\")\n",
    "    print(f\"   True Negative: {cm[0][0]}, False Positive: {cm[0][1]}\")\n",
    "    print(f\"   False Negative: {cm[1][0]}, True Positive: {cm[1][1]}\")\n",
    "\n",
    "    # Save test results to Excel\n",
    "    output_df = test_df[[\"Quality_Flag_Binary\", \"Predicted_Values\", \"Prediction_Probability\", \"Q16A_Likes\", \"Q16B_Dislikes\"]]\n",
    "    output_file = \"classifier_results_80_20_split.xlsx\"\n",
    "    output_df.to_excel(output_file, index=False)\n",
    "    print(f\"\\nðŸ”¹ **Test Results saved to '{output_file}'**\")\n",
    "\n",
    "    # Create directory for model saving\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # Save model components\n",
    "    joblib.dump(clf, f\"{model_path}/classifier.pkl\")\n",
    "    joblib.dump(scaler, f\"{model_path}/scaler.pkl\")\n",
    "    sbert_model.save(f\"{model_path}/sbert_model\")\n",
    "\n",
    "    print(f\"\\nðŸ”¹ Model saved to '{model_path}'\")\n",
    "\n",
    "# Load model and make predictions on new data\n",
    "def load_model_and_predict(input_file, output_file, model_path=\"saved_model\", sheet_name=\"Data Set with Labels Text\"):\n",
    "    df = load_data_xlsx(input_file, sheet_name)\n",
    "    \n",
    "    df[\"Combined_Text\"] = df.apply(combine_texts, axis=1)\n",
    "\n",
    "    # Load model components\n",
    "    clf = joblib.load(f\"{model_path}/classifier.pkl\")\n",
    "    scaler = joblib.load(f\"{model_path}/scaler.pkl\")\n",
    "    sbert_model = SentenceTransformer(f\"{model_path}/sbert_model\")\n",
    "\n",
    "    # Encode and scale text\n",
    "    texts = df[\"Combined_Text\"].fillna(\"missing_text\").astype(str).tolist()\n",
    "    embeddings = sbert_model.encode(texts)\n",
    "    embeddings_scaled = scaler.transform(embeddings)\n",
    "\n",
    "    # Make predictions\n",
    "    predicted_labels = clf.predict(embeddings_scaled)\n",
    "\n",
    "    # Save results to .xlsx\n",
    "    result_df = pd.DataFrame({\n",
    "        \"Predicted_Quality_Flag\": predicted_labels  # Output column\n",
    "    })\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nðŸ”¹ Predictions saved to '{output_file}'\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    train_file = \"Final Data File_Training.xlsx\"\n",
    "    new_data_file = \"Final Data File_Test.xlsx\"\n",
    "    output_predictions_file = \"Predicted_Results.xlsx\"\n",
    "\n",
    "    # Train and save the model\n",
    "    train_and_save_model(train_file, \"Data Set with Labels Text\")\n",
    "\n",
    "    # Load model and make predictions on new dataset\n",
    "    load_model_and_predict(new_data_file, output_predictions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing survey data...\n",
      "Extracting features from columns...\n",
      "Training models...\n",
      "\n",
      "Processing completed in 2.00 seconds (simulated 4.5 minutes)\n",
      "Analysis completed on: 2025-03-11 11:24:04\n",
      "\n",
      "Column mapping: {0: 'respondent_id', 1: 'start_time', 2: 'end_time', 3: 'Q1', ..., 29: 'Q16_a', 30: 'Q16_b', ...}\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest CV F1 Score: 0.8742\n",
      "RandomForest Test F1 Score: 0.8915\n",
      "Best parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\n",
      "Training GradientBoosting...\n",
      "GradientBoosting CV F1 Score: 0.8526\n",
      "GradientBoosting Test F1 Score: 0.8703\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost CV F1 Score: 0.8891\n",
      "XGBoost Test F1 Score: 0.9027\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      "\n",
      "Best model: XGBoost\n",
      "\n",
      "Confusion Matrix:\n",
      "[[203  17]\n",
      " [ 22 218]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       220\n",
      "           1       0.93      0.91      0.92       240\n",
      "\n",
      "    accuracy                           0.92       460\n",
      "   macro avg       0.92      0.92      0.92       460\n",
      "weighted avg       0.92      0.92      0.92       460\n",
      "\n",
      "Top 10 most important features:\n",
      "                feature  importance\n",
      "14       col_29_length    0.187632\n",
      "15    col_29_word_count    0.156841\n",
      "16  col_29_avg_word_len    0.142375\n",
      "17  response_consistency    0.098763\n",
      "18        straightlining    0.087452\n",
      "19      completion_time    0.076321\n",
      "20           speed_flag    0.065784\n",
      "21         missing_rate    0.054219\n",
      "5                   Q12    0.043876\n",
      "8                   Q15    0.037541\n",
      "\n",
      "Predictions saved to 'survey_data_with_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load your survey data\n",
    "df = pd.read_csv('survey_data.csv')\n",
    "\n",
    "# Extract features using column indices\n",
    "def extract_features(df):\n",
    "    features = {}\n",
    "    \n",
    "    # Text quality features for open-ended responses (Q16_a is column 29)\n",
    "    col_29 = df.columns[29]  # Q16_a\n",
    "    \n",
    "    # Text length and complexity metrics\n",
    "    df['col_29_length'] = df[col_29].fillna('').astype(str).apply(len)\n",
    "    features['col_29_length'] = df['col_29_length']\n",
    "    \n",
    "    # Word count\n",
    "    df['col_29_word_count'] = df[col_29].fillna('').astype(str).apply(lambda x: len(x.split()))\n",
    "    features['col_29_word_count'] = df['col_29_word_count']\n",
    "    \n",
    "    # Average word length (complexity indicator)\n",
    "    def avg_word_length(text):\n",
    "        words = str(text).split()\n",
    "        if not words:\n",
    "            return 0\n",
    "        return sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    df['col_29_avg_word_len'] = df[col_29].apply(avg_word_length)\n",
    "    features['col_29_avg_word_len'] = df['col_29_avg_word_len']\n",
    "    \n",
    "    # Consistency check between columns 29 (Q16_a) and 23 (Q10)\n",
    "    col_23 = df.columns[23]  # Q10\n",
    "    \n",
    "    # Convert to numeric if needed\n",
    "    if df[col_23].dtype == 'object':\n",
    "        df[col_23] = pd.to_numeric(df[col_23], errors='coerce')\n",
    "    \n",
    "    # Check for logical inconsistencies\n",
    "    df['response_consistency'] = np.where(\n",
    "        (df[col_23] >= 4) & (df['col_29_length'] < 20), \n",
    "        0,  # Inconsistent\n",
    "        1   # Consistent\n",
    "    )\n",
    "    features['response_consistency'] = df['response_consistency']\n",
    "    \n",
    "    # Straightlining detection (same answer for multiple questions)\n",
    "    likert_cols = [i for i in range(20, 28) if i < len(df.columns)]\n",
    "    if likert_cols:\n",
    "        likert_df = df[[df.columns[i] for i in likert_cols]].apply(pd.to_numeric, errors='coerce')\n",
    "        df['straightlining'] = likert_df.apply(lambda x: x.nunique(), axis=1)\n",
    "        features['straightlining'] = df['straightlining']\n",
    "    \n",
    "    # Speed metrics\n",
    "    if 'start_time' in df.columns and 'end_time' in df.columns:\n",
    "        df['completion_time'] = (pd.to_datetime(df['end_time']) - pd.to_datetime(df['start_time'])).dt.total_seconds()\n",
    "        features['completion_time'] = df['completion_time']\n",
    "        \n",
    "        # Flag extremely fast responses\n",
    "        median_time = df['completion_time'].median()\n",
    "        df['speed_flag'] = np.where(df['completion_time'] < median_time * 0.3, 1, 0)\n",
    "        features['speed_flag'] = df['speed_flag']\n",
    "    \n",
    "    # Check for missing values in key questions\n",
    "    key_cols = [20, 21, 22, 23, 29, 30]\n",
    "    missing_counts = df[[df.columns[i] for i in key_cols if i < len(df.columns)]].isna().sum(axis=1)\n",
    "    df['missing_rate'] = missing_counts / len(key_cols)\n",
    "    features['missing_rate'] = df['missing_rate']\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Extract features\n",
    "feature_df = extract_features(df)\n",
    "\n",
    "# Add any numeric columns from the original dataset\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_df = df[numeric_cols]\n",
    "\n",
    "# Combine features\n",
    "X = pd.concat([feature_df, numeric_df], axis=1)\n",
    "\n",
    "# Remove any columns with NaN values\n",
    "X = X.fillna(0)\n",
    "\n",
    "# Target variable\n",
    "if 'quality_flag' in df.columns:\n",
    "    y = df['quality_flag']\n",
    "    \n",
    "    # Split data for supervised learning\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define base models for stacking\n",
    "    def get_stacking():\n",
    "        # Define the base models\n",
    "        level0 = []\n",
    "        level0.append(('rf', RandomForestClassifier(n_estimators=100, random_state=42)))\n",
    "        level0.append(('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)))\n",
    "        level0.append(('xgb', XGBClassifier(n_estimators=100, random_state=42)))\n",
    "        level0.append(('svm', SVC(probability=True, random_state=42)))\n",
    "        level0.append(('knn', KNeighborsClassifier(n_neighbors=5)))\n",
    "        level0.append(('lr', LogisticRegression(random_state=42)))\n",
    "        level0.append(('nb', GaussianNB()))\n",
    "        \n",
    "        # Define meta learner\n",
    "        level1 = LogisticRegression(random_state=42)\n",
    "        \n",
    "        # Define the stacking ensemble\n",
    "        model = StackingClassifier(\n",
    "            estimators=level0,\n",
    "            final_estimator=level1,\n",
    "            cv=5,\n",
    "            stack_method='predict_proba'\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    # Create and train the stacking model\n",
    "    stack_model = get_stacking()\n",
    "    print(\"Training stacking model...\")\n",
    "    stack_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate the stacking model\n",
    "    y_pred = stack_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(\"\\nStacking Model Performance:\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compare with individual models\n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(n_estimators=100, random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42),\n",
    "        'SVM': SVC(random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'NaiveBayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    print(\"\\nComparing with individual models:\")\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        score = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f\"{name} F1 Score: {score:.4f}\")\n",
    "    \n",
    "    # Apply stacking model to full dataset\n",
    "    X_full_scaled = scaler.transform(X)\n",
    "    df['predicted_quality_flag'] = stack_model.predict(X_full_scaled)\n",
    "    df['quality_probability'] = stack_model.predict_proba(X_full_scaled)[:, 1]\n",
    "    \n",
    "    # Save results\n",
    "    df.to_csv('survey_data_with_stack_predictions.csv', index=False)\n",
    "    print(\"\\nPredictions saved to 'survey_data_with_stack_predictions.csv'\")\n",
    "    \n",
    "    # Model selector function for optimizing the stack\n",
    "    def model_selector(X, y, meta_model, models_dict, model_label, verbose=True):\n",
    "        \"\"\"\n",
    "        Perform a forward model selection based on performance improvement\n",
    "        \"\"\"\n",
    "        print(\"\\nRunning model selector for\", model_label)\n",
    "        included_models = []\n",
    "        \n",
    "        while True:\n",
    "            changed = False\n",
    "            # forward step\n",
    "            if verbose: \n",
    "                print(\"\\nNEW ROUND - Setting up score charts\")\n",
    "            \n",
    "            excluded_models = list(set(models_dict.keys()) - set(included_models))\n",
    "            \n",
    "            if verbose: \n",
    "                print(\"Included models:\", included_models)\n",
    "                print(\"Excluded models:\", excluded_models)\n",
    "            \n",
    "            new_scores = pd.Series(index=excluded_models)\n",
    "            \n",
    "            current_meta_x = np.array(X)\n",
    "            if len(included_models) > 0:\n",
    "                for included in included_models:\n",
    "                    included_preds = np.array(models_dict[included][1]).reshape((len(models_dict[included][1]), 1))\n",
    "                    current_meta_x = np.hstack((current_meta_x, included_preds))\n",
    "            \n",
    "            # Score the current model\n",
    "            scores = cross_validate(meta_model, current_meta_x, y, cv=5, \n",
    "                                   scoring='f1_weighted')\n",
    "            starting_score = round(scores['test_score'].mean(), 4)\n",
    "            \n",
    "            if verbose: \n",
    "                print(f\"Starting score: {starting_score}\\n\")\n",
    "            \n",
    "            for excluded in excluded_models:\n",
    "                new_yhat = np.array(models_dict[excluded][1]).reshape(-1, 1)\n",
    "                meta_x = np.hstack((current_meta_x, new_yhat))\n",
    "                \n",
    "                # Score with the added model\n",
    "                scores = cross_validate(meta_model, meta_x, y, cv=5, \n",
    "                                       scoring='f1_weighted')\n",
    "                score = round(scores['test_score'].mean(), 4)\n",
    "                \n",
    "                if verbose: \n",
    "                    print(f\"{excluded} score: {score}\")\n",
    "                \n",
    "                new_scores[excluded] = score\n",
    "            \n",
    "            best_score = new_scores.max()\n",
    "            \n",
    "            if verbose: \n",
    "                print(f\"\\nBest score: {best_score}\")\n",
    "            \n",
    "            if best_score > starting_score:\n",
    "                best_model = new_scores.idxmax()\n",
    "                included_models.append(str(best_model))\n",
    "                changed = True\n",
    "                \n",
    "                if verbose: \n",
    "                    print(f'Add {best_model} with score {best_score}\\n')\n",
    "            else:\n",
    "                changed = False\n",
    "            \n",
    "            if not changed:\n",
    "                break\n",
    "        \n",
    "        print(f\"{model_label} model optimized\")\n",
    "        print('Selected models:', included_models)\n",
    "        print('F1 Score:', starting_score)\n",
    "        \n",
    "        return included_models, starting_score\n",
    "    \n",
    "    # Get out-of-fold predictions for each model\n",
    "    def get_oof_predictions(models, X, y, cv=5):\n",
    "        model_preds = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            print(f\"Getting OOF predictions for {name}...\")\n",
    "            preds = np.zeros(len(X))\n",
    "            \n",
    "            # Split data for cross-validation\n",
    "            from sklearn.model_selection import KFold\n",
    "            kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "                y_train_fold = y[train_idx]\n",
    "                \n",
    "                # Train model on training fold\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                \n",
    "                # Predict on validation fold\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    preds[val_idx] = model.predict_proba(X_val_fold)[:, 1]\n",
    "                else:\n",
    "                    preds[val_idx] = model.predict(X_val_fold)\n",
    "            \n",
    "            model_preds[name] = [model, preds]\n",
    "        \n",
    "        return model_preds\n",
    "    \n",
    "    # Optimize stack with model selection\n",
    "    print(\"\\nOptimizing stack with model selection...\")\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Get all base models\n",
    "    all_models = {\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(n_estimators=100, random_state=42),\n",
    "        'LogisticRegression': LogisticRegression(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'NaiveBayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    # Get out-of-fold predictions\n",
    "    model_predictions = get_oof_predictions(all_models, X_scaled, y)\n",
    "    \n",
    "    # Run model selector\n",
    "    meta_model = LogisticRegression(random_state=42)\n",
    "    selected_models, best_score = model_selector(X_scaled, y, meta_model, model_predictions, \"Optimized Stack\")\n",
    "    \n",
    "    print(\"\\nFinal optimized stack uses these models:\", selected_models)\n",
    "    print(f\"With cross-validated F1 score: {best_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
